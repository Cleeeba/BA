{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import zscore\n",
    "import gzip\n",
    "import os\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from alive_progress import alive_bar\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import pandas as pd\n",
    "\n",
    "start_date = 1800\n",
    "end_date = 2000\n",
    "numbers = list(range(start_date,end_date))\n",
    "\n",
    "conf= SparkConf().setAll([('spark.executor.memory', '16g'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.driver.memory','16g')])\n",
    "spark = SparkSession.builder.config(conf=conf).appName('NgramSQL').getOrCreate()\n",
    "\n",
    "#spark = SparkSession.builder.appName('3gramSQL').getOrCreate()\n",
    "df_2gram = spark.read.parquet(\"C:/Users/bincl/BA-Thesis/Dataset/2gram/warehouse/2gram_table\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pd_df(df):\n",
    "    matched_pandas_df = pd.Series(df).astype(int).reindex(numbers, fill_value=0)\n",
    "    return matched_pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLR(full,left,right):\n",
    "    X = pd.concat([left,right],axis=1)\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X, full) \n",
    "    return reg.coef_,reg.intercept_,X\n",
    "\n",
    "def buildApproximation(c1, c2, basevalue, df):\n",
    "    df = df.fillna(0)\n",
    "    df['approximation'] = c1 * df.iloc[:, 0] + c2 * df.iloc[:, 1] + basevalue\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compressWithError2gram(chunk_df,error):\n",
    "    df_2_gram = chunk_df.collect()\n",
    "    n = len(df_2_gram)\n",
    "    print(\"got it\")\n",
    "    result = []\n",
    "    all = []\n",
    "    sum = []\n",
    "    \n",
    "    with alive_bar(n,length= 20, force_tty = True, bar = 'smooth') as bar:\n",
    "        for i in range(n):\n",
    "            df_file = df_2_gram[i]\n",
    "            full = get_pd_df(df_file['Frequency_N'])\n",
    "            left = get_pd_df(df_file['Frequency_L'])\n",
    "            right =  get_pd_df(df_file['Frequency_R'])\n",
    "            if not(full.eq(0).all() or right.eq(0).all() or left.eq(0).all()):\n",
    "                coef,intercept,dfAprox = MLR(full,left,right)\n",
    "                c1,c2 = coef\n",
    "                df = buildApproximation(c1,c2,intercept,dfAprox)\n",
    "                dfOriginal = pd.DataFrame({'values': pd.to_numeric(full), 'zscore': zscore(full)})\n",
    "                df['zscore'] = zscore(df['approximation'])\n",
    "                if not(df.isnull().values.any()):\n",
    "                    sum.append(pd.to_numeric(dfOriginal['values']).sum()) \n",
    "                    rmse = mean_squared_error(dfOriginal['zscore'], df['zscore'], squared = False)\n",
    "                    if rmse <= error:\n",
    "                        result.append([rmse,dfOriginal['values'],dfOriginal['zscore'],df['approximation'],df['zscore']]) \n",
    "                    all.append(rmse)    \n",
    "            bar()\n",
    "    return result, all, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result, all, sum = compressWithError2gram(10, 1)\n",
    "\n",
    "#print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box(all,result):\n",
    "    n = 0\n",
    "    plt.boxplot(all)\n",
    "    plt.show()\n",
    "    rmse_with_error = []\n",
    "\n",
    "    for i in result:\n",
    "        rmse_with_error.append(i[0])\n",
    "    plt.boxplot(rmse_with_error)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    sns.violinplot(x= rmse_with_error, inner=\"point\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line(result):\n",
    "    n=0\n",
    "    result[n][1].plot()\n",
    "    result[n][3].plot()\n",
    "    plt.show()\n",
    "    result[n][2].plot()\n",
    "    result[n][4].plot()\n",
    "    plt.show()\n",
    "    print(result[n][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter(sum, all):\n",
    "    sns.violinplot(x= all, inner_kws=dict(box_width=15, whis_width=2, color=\".8\"))\n",
    "    plt.show()\n",
    "    plt.scatter(sum, all)\n",
    "    plt.xscale('log')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got it\n",
      "|██▍                 | ▆█▆ 26816/229998 [12%] in 2:08 (~16:12, 209.1/s)  ▅▇▇ 22225/229998 [10%] in 1:46 (~16:34, 209.0/s) "
     ]
    }
   ],
   "source": [
    "\n",
    "total_rows = df_2gram.count()\n",
    "\n",
    "data_result=[]\n",
    "data_all= []\n",
    "data_sum = []\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "sample_dict = {}\n",
    "# add the partition_number as a column\n",
    "df = df_2gram.withColumn('partition_num', F.spark_partition_id())\n",
    "\n",
    "total_partition = [int(row.partition_num) for row in \n",
    "df.select('partition_num').distinct().collect()]\n",
    "\n",
    "for each_df in total_partition:\n",
    "    sample_dict[each_df] = df.where(df.partition_num == each_df) \n",
    "    \n",
    "\n",
    "for i in range(0,len(sample_dict)):\n",
    "    result, all, sum = compressWithError2gram(sample_dict[i], 0.5)\n",
    "    print(i)\n",
    "    \n",
    "    plt.close(\"all\")\n",
    "    data_result.extend(result)\n",
    "    data_all.extend(all)\n",
    "    data_sum.extend(sum)\n",
    "    box(data_all,data_result)\n",
    "    line(data_result)\n",
    "    scatter(data_sum,data_all)\n",
    "    \n",
    "    print(len(result))\n",
    "    \n",
    "    \n",
    "                \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
