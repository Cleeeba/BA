{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a31ea07-2865-4cec-906c-e4483978098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('CorpusLoader').master('local[4]').config('spark.driver.memory', '8g').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9afdcd10-2c3d-4889-b37a-2488dda27a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import split, element_at, explode, map_values, array_min, broadcast, map_from_entries, arrays_zip, array_contains, monotonically_increasing_id, array_distinct, transform, arrays_zip, size, slice, collect_list, first, map_from_arrays\n",
    "from pyspark.sql.types import LongType, ArrayType, IntegerType, MapType\n",
    "\n",
    "class CorpusLoader:\n",
    "\n",
    "    def __init__(self, root_path, spark):\n",
    "        self.__root_path = root_path\n",
    "        self.__spark = spark\n",
    "\n",
    "    def load(self):\n",
    "        self.__array_df = self.__load_or_create_parquet('array.parquet', self.__create_array_df)\n",
    "        self.__token_df = self.__load_or_create_parquet('token.parquet', self.__create_token_df)\n",
    "        self.__contains_df = self.__load_or_create_parquet('contains.parquet', self.__create_contains_df)\n",
    "        self.__data_df = self.__load_or_create_parquet('data.parquet', self.__create_data_df)\n",
    "\n",
    "    def __load_or_create_parquet(self, name, create_function):\n",
    "        parquet_path = os.path.join(os.path.join(self.__root_path, 'parquets'), name)\n",
    "        \n",
    "        if not os.path.exists(parquet_path):\n",
    "            print(f'File \"{name}\" not found. \\n\\t -- Creating \"{name}\" ...')\n",
    "            \n",
    "            df = create_function()\n",
    "            df.write.parquet(parquet_path)\n",
    "\n",
    "            print('\\t -- Done.')\n",
    "\n",
    "        print(f'Loading \"{name}\" ...')\n",
    "        return self.__spark.read.parquet(parquet_path)\n",
    "\n",
    "    def __create_token_df(self):\n",
    "        one_gram_path = os.path.join(self.__root_path, '1')\n",
    "\n",
    "        one_gram_df = spark.read.csv(one_gram_path, sep='\\n', quote=\"\").withColumnRenamed('_c0', 'Input')\n",
    "        token_df = one_gram_df \\\n",
    "                .select(split('Input', '\\t').alias('SplitInput')) \\\n",
    "                .select(element_at('SplitInput', 1).alias('Tokens')) \\\n",
    "                .select(explode(split('Tokens', ' ')).alias('Token')) \\\n",
    "                .orderBy('Token') \\\n",
    "                .withColumn('TokenId', monotonically_increasing_id()) \n",
    "        \n",
    "        return token_df\n",
    "\n",
    "    def __create_array_df(self):\n",
    "        n_gram_directories = [os.path.join(self.__root_path, x) for x in os.listdir(self.__root_path) if x.isdigit()]\n",
    "        \n",
    "        input_df = None\n",
    "\n",
    "        for path in n_gram_directories:\n",
    "            new_input_df = spark.read.csv(path, sep='\\n', quote=\"\").withColumnRenamed('_c0', 'Input')\n",
    "            \n",
    "            if input_df is None:\n",
    "                input_df = new_input_df\n",
    "            else:\n",
    "                input_df = input_df.union(new_input_df)\n",
    "\n",
    "        split_df = input_df \\\n",
    "                    .select(split('Input', '\\t').alias('SplitInput')) \\\n",
    "                    .select(element_at('SplitInput', 1).alias('Tokens'),\n",
    "                            slice('SplitInput', 2, size('SplitInput')).alias('Data')) \\\n",
    "                    .select(split('Tokens', ' ').alias('Tokens'), 'Data')\n",
    "\n",
    "        array_df = split_df.select('Tokens', transform('Data', lambda d: split(d, ',')).alias('Data')) \\\n",
    "                    .select('Tokens', transform('Data', lambda x: x[0].cast(IntegerType())).alias('Years'),\n",
    "                            transform('Data', lambda x: x[1].cast(LongType())).alias('Frequency'),\n",
    "                            transform('Data', lambda x: x[2].cast(LongType())).alias('BookFrequency')) \\\n",
    "                    .withColumn('NgramId', monotonically_increasing_id())\n",
    "\n",
    "        return array_df\n",
    "\n",
    "    def __create_contains_df(self):\n",
    "        n_gram_df = self.__array_df\n",
    "\n",
    "        n_gram_to_token_id_df = n_gram_df.select('NgramId', 'Tokens') \\\n",
    "                .select(explode('Tokens').alias('Token'), 'NgramId') \\\n",
    "                .join(self.__token_df, on='Token') \\\n",
    "                .groupBy('NgramId').agg(collect_list('TokenId').alias('TokenIds'))\n",
    "        print(n_gram_to_token_id_df.count())\n",
    "\n",
    "        contains_df = n_gram_to_token_id_df.select('NgramId', 'TokenIds') \\\n",
    "            .withColumn('IndexArray', transform('TokenIds', lambda x, i: i)) \\\n",
    "            .select('NgramId', arrays_zip('IndexArray', 'TokenIds').alias('TokenIds')) \\\n",
    "            .select('NgramId', explode('TokenIds').alias('TokenId')) \\\n",
    "            .select('NgramId', 'TokenId.IndexArray', 'TokenId.TokenIds') \\\n",
    "            .withColumnsRenamed({'IndexArray': 'Position', 'TokenIds': 'TokenId'}) \\\n",
    "            .orderBy('NgramId')\n",
    "        print(contains_df.count())\n",
    "\n",
    "        return contains_df\n",
    "\n",
    "    ## This horrific arrays to list of structs to map construct is required, because map_from_arrays zeroes everything out.\n",
    "    def __create_data_df(self):\n",
    "        data_df = self.__array_df.select('NgramId', 'Years', 'Frequency', 'BookFrequency')\n",
    "        data_df = data_df.withColumn('FrequencyStructs', arrays_zip('Years', 'Frequency'))\n",
    "        data_df = data_df.withColumn('BookFrequencyStructs', arrays_zip('Years', 'BookFrequency'))\n",
    "        data_df = data_df.withColumn('FrequencyMap', map_from_entries('FrequencyStructs'))\n",
    "        data_df = data_df.withColumn('BookFrequencyMap', map_from_entries('BookFrequencyStructs'))\n",
    "        data_df = data_df.select('NgramId', 'FrequencyMap', 'BookFrequencyMap')\n",
    "\n",
    "        data_df.printSchema()\n",
    "        \n",
    "        return data_df.withColumnsRenamed({'FrequencyMap': 'Frequency', 'BookFrequencyMap': 'BookFrequency'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3926ed59-4eda-41c0-9f33-cb92eae695d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading \"array.parquet\" ...\n",
      "Loading \"token.parquet\" ...\n",
      "Loading \"contains.parquet\" ...\n",
      "Loading \"data.parquet\" ...\n"
     ]
    }
   ],
   "source": [
    "cl = CorpusLoader('C:/Users/bincl/BA-Thesis/Dataset/parquets_corpus/', spark)\n",
    "\n",
    "cl.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e433bf8f-33b2-45a6-b293-16321386ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----------------+------------------+----------------+--------+-----------------+--------+\n",
      "|      NgramId|     Ngram|Length|LeftChildTokenIds|RightChildTokenIds|LeftChildNgramId|LCTokens|RightChildNgramId|RCTokens|\n",
      "+-------------+----------+------+-----------------+------------------+----------------+--------+-----------------+--------+\n",
      "|1932735304174|  [26, 26]|     2|             [26]|              [26]|            3879|    [26]|             3879|    [26]|\n",
      "|1924145471799|  [29, 26]|     2|             [29]|              [26]|            3705|    [29]|             3879|    [26]|\n",
      "| 352187426737| [474, 26]|     2|            [474]|              [26]|            3785|   [474]|             3879|    [26]|\n",
      "| 214748505745| [964, 26]|     2|            [964]|              [26]|            3860|   [964]|             3879|    [26]|\n",
      "| 128849131373|[1677, 26]|     2|           [1677]|              [26]|            3688|  [1677]|             3879|    [26]|\n",
      "| 120259226401|[1697, 26]|     2|           [1697]|              [26]|            3703|  [1697]|             3879|    [26]|\n",
      "|1365799641803|[1806, 26]|     2|           [1806]|              [26]|            3741|  [1806]|             3879|    [26]|\n",
      "|1425929268117|[1950, 26]|     2|           [1950]|              [26]|             464|  [1950]|             3879|    [26]|\n",
      "| 506806310159|[2040, 26]|     2|           [2040]|              [26]|             197|  [2040]|             3879|    [26]|\n",
      "|1090921827076|[2214, 26]|     2|           [2214]|              [26]|            1696|  [2214]|             3879|    [26]|\n",
      "|1108101572635|[2250, 26]|     2|           [2250]|              [26]|            1125|  [2250]|             3879|    [26]|\n",
      "|1400159351595|[2453, 26]|     2|           [2453]|              [26]|            1677|  [2453]|             3879|    [26]|\n",
      "|1400159406945|[2509, 26]|     2|           [2509]|              [26]|             685|  [2509]|             3879|    [26]|\n",
      "|1408749285871|[2529, 26]|     2|           [2529]|              [26]|            1765|  [2529]|             3879|    [26]|\n",
      "|1949915339020|[2927, 26]|     2|           [2927]|              [26]|            1452|  [2927]|             3879|    [26]|\n",
      "|1005022493869|[3091, 26]|     2|           [3091]|              [26]|             266|  [3091]|             3879|    [26]|\n",
      "| 326417541462|[3506, 26]|     2|           [3506]|              [26]|            1500|  [3506]|             3879|    [26]|\n",
      "|1434519161803|[3764, 26]|     2|           [3764]|              [26]|            1232|  [3764]|             3879|    [26]|\n",
      "|1451699013771|[4590, 26]|     2|           [4590]|              [26]|     17179872954|  [4590]|             3879|    [26]|\n",
      "|1743756835375|[4823, 26]|     2|           [4823]|              [26]|     17179872102|  [4823]|             3879|    [26]|\n",
      "+-------------+----------+------+-----------------+------------------+----------------+--------+-----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_array_df = cl._CorpusLoader__contains_df.orderBy('NgramId', 'Position').groupBy('NgramId').agg(collect_list('TokenId').alias('Tokens'))\n",
    "df = token_array_df.withColumnRenamed('Tokens', 'Ngram').withColumn('Length', size('Ngram')).where('Length > 1')\n",
    "#df.cache()\n",
    "\n",
    "df = df.withColumn('LeftChildTokenIds', slice(df.Ngram, 1, df.Length - 1))\n",
    "df = df.withColumn('RightChildTokenIds', slice(df.Ngram, 2,df.Length - 1))\n",
    "\n",
    "result = df.join(token_array_df.withColumnRenamed('NgramId', 'LeftChildNgramId'), on=df.LeftChildTokenIds == token_array_df.Tokens).withColumnRenamed('Tokens', 'LCTokens')\n",
    "\n",
    "table = result.join(token_array_df.withColumnRenamed('NgramId', 'RightChildNgramId'), on=df.RightChildTokenIds == token_array_df.Tokens).withColumnRenamed('Tokens', 'RCTokens').cache()\n",
    "\n",
    "table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d063acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------------+\n",
      "|      NgramId|LeftChildNgramId|RightChildNgramId|\n",
      "+-------------+----------------+-----------------+\n",
      "|1932735304174|            3879|             3879|\n",
      "|1924145471799|            3705|             3879|\n",
      "| 352187426737|            3785|             3879|\n",
      "| 214748505745|            3860|             3879|\n",
      "| 128849131373|            3688|             3879|\n",
      "| 120259226401|            3703|             3879|\n",
      "|1365799641803|            3741|             3879|\n",
      "|1425929268117|             464|             3879|\n",
      "| 506806310159|             197|             3879|\n",
      "|1090921827076|            1696|             3879|\n",
      "|1108101572635|            1125|             3879|\n",
      "|1400159351595|            1677|             3879|\n",
      "|1400159406945|             685|             3879|\n",
      "|1408749285871|            1765|             3879|\n",
      "|1949915339020|            1452|             3879|\n",
      "|1005022493869|             266|             3879|\n",
      "| 326417541462|            1500|             3879|\n",
      "|1434519161803|            1232|             3879|\n",
      "|1451699013771|     17179872954|             3879|\n",
      "|1743756835375|     17179872102|             3879|\n",
      "+-------------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = table.select(\"NgramId\",\"LeftChildNgramId\",\"RightChildNgramId\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19d175f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NgramId: long (nullable = true)\n",
      " |-- Frequency: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: long (valueContainsNull = true)\n",
      " |-- BookFrequency: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: long (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cl._CorpusLoader__data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee732a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "efe866aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         Frequency_N|         Frequency_L|         Frequency_R|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|{1845 -> 1, 1964 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "|{1732 -> 1, 1780 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "|{1890 -> 1, 1894 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "|{1857 -> 2, 1858 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "|{1818 -> 1, 1863 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "|{1860 -> 2, 1862 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "|{1904 -> 3, 1905 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "|{1857 -> 2, 1862 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "|{1812 -> 7, 1818 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "|{1851 -> 1, 1852 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "|{1702 -> 1, 1720 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "|{1765 -> 1, 1831 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "|{1836 -> 1, 1837 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "|{1862 -> 2, 1874 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "|{1750 -> 1, 1783 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "|{1767 -> 1, 1770 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "|{1862 -> 2, 1882 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "|{1795 -> 1, 1838 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "|{1868 -> 1, 1908 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "|{1864 -> 3, 1887 ...|{1522 -> 3, 1533 ...|{1522 -> 3, 1533 ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "#1000 limit is 3m 30\n",
    "result = result.join(cl._CorpusLoader__data_df, on=(\"NgramId\")).withColumnRenamed(\"Frequency\",\"Frequency_N\")\n",
    "result = result.join(cl._CorpusLoader__data_df.alias(\"dataL\"),(col(\"LeftChildNgramId\") == col(\"dataL.NgramId\"))).withColumnRenamed(\"Frequency\",\"Frequency_L\")\n",
    "ngram_table = result.join(cl._CorpusLoader__data_df.alias(\"dataR\"),(col(\"LeftChildNgramId\") == col(\"dataR.NgramId\"))).withColumnRenamed(\"Frequency\",\"Frequency_R\")\n",
    "ngram_table= ngram_table.select(\"Frequency_N\",\"Frequency_L\",\"Frequency_R\")\n",
    "ngram_table.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
