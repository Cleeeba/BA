{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col, element_at, slice, size, regexp_extract, transform, when, explode, \\\n",
    "monotonically_increasing_id, map_from_arrays, lit, udf,collect_list, row_number,aggregate, ceil, map_keys, expr, from_json, sum\n",
    "from pyspark.sql.types import ShortType, ArrayType, LongType, StringType\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#path = 'C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_ab.gz'\n",
    "start_date = 1800\n",
    "end_date = 2000\n",
    "path = 'C:/Users/bincl/BA-Thesis/Dataset/3gram/default/3_20000_nopos_sample/3_20000_nopos_sample.gz'\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('3gramSQL').getOrCreate()\n",
    "\n",
    "raw_input_df = spark \\\n",
    "                .read.csv(path, sep='\\n',quote=\"\").withColumnRenamed('_c0', 'Input')\n",
    "\n",
    "split_df = raw_input_df \\\n",
    "                .select(split('Input', '\\t').alias('SplitInput')) \\\n",
    "                .select(element_at('SplitInput', 1).alias('Tokens'),\n",
    "                                        slice('SplitInput', 2,\n",
    "                size('SplitInput')).alias('Data')) \\\n",
    "                                .select('Tokens', 'Data') \\\n",
    "\n",
    "test_df_3gram = split_df.select('Tokens', transform('Data', lambda d:\n",
    "                split(d, ',')).alias('Data')) \\\n",
    "                                .select('Tokens', transform('Data', lambda x:\n",
    "                x[0]).alias('Year'),\n",
    "                                        transform('Data', lambda x:\n",
    "                x[1]).cast(ArrayType(LongType())).alias('Occurrences')) \n",
    "                            \n",
    "df = test_df_3gram.withColumn(\n",
    "    \"Sum\",\n",
    "        aggregate(\"Occurrences\", lit(0), lambda acc, x: (acc + x).cast(\"int\"))\n",
    ")\n",
    "df_3gram =  df.select('Tokens', map_from_arrays('Year',\n",
    "                'Occurrences').alias('Data') ,'Sum') \\\n",
    "                                .select(['Tokens', 'Data', 'Sum'])  \n",
    "df_3gram = df_3gram.repartition(5)                            \n",
    "                     \n",
    "df_3gram.sort(\"Sum\").write.parquet(\"C:/Users/bincl/BA-Thesis/Dataset/3gram/parquet/3_20000_nopos_sample/3_20000_nopos_sample.gz\", mode= 'overwrite')                          \n",
    "    \n",
    "df= spark.read.parquet(\"C:/Users/bincl/BA-Thesis/Dataset/3gram/parquet/3_20000_nopos_sample/3_20000_nopos_sample.gz\")\n",
    "print(df.head(5))\n",
    "print(df.tail(5))                            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
