{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('CorpusLoader').master('local[4]').config(\"spark.executor.memory\", \"8g\").getOrCreate()\n",
    "import os\n",
    "from pyspark.sql.functions import split, element_at, explode, map_values, array_min, broadcast, map_from_entries, arrays_zip, array_contains, monotonically_increasing_id, array_distinct, transform, arrays_zip, size, slice, collect_list, first, map_from_arrays\n",
    "from pyspark.sql.types import LongType, ArrayType, IntegerType, MapType\n",
    "\n",
    "class CorpusLoader:\n",
    "\n",
    "    def __init__(self, root_path, spark):\n",
    "        self.__root_path = root_path\n",
    "        self.__spark = spark\n",
    "\n",
    "    def load(self):\n",
    "        self.__array_df = self.__load_or_create_parquet('array.parquet', self.__create_array_df)\n",
    "        self.__token_df = self.__load_or_create_parquet('token.parquet', self.__create_token_df)\n",
    "        self.__contains_df = self.__load_or_create_parquet('contains.parquet', self.__create_contains_df)\n",
    "        self.__data_df = self.__load_or_create_parquet('data.parquet', self.__create_data_df)\n",
    "\n",
    "    def __load_or_create_parquet(self, name, create_function):\n",
    "        parquet_path = os.path.join(os.path.join(self.__root_path, 'parquets'), name)\n",
    "        \n",
    "        if not os.path.exists(parquet_path):\n",
    "            print(f'File \"{name}\" not found. \\n\\t -- Creating \"{name}\" ...')\n",
    "            \n",
    "            df = create_function()\n",
    "            df.write.parquet(parquet_path)\n",
    "\n",
    "            print('\\t -- Done.')\n",
    "\n",
    "        print(f'Loading \"{name}\" ...')\n",
    "        return self.__spark.read.parquet(parquet_path)\n",
    "\n",
    "    def __create_token_df(self):\n",
    "        one_gram_path = os.path.join(self.__root_path, '1')\n",
    "\n",
    "        one_gram_df = spark.read.csv(one_gram_path, sep='\\n').withColumnRenamed('_c0', 'Input')\n",
    "        token_df = one_gram_df \\\n",
    "                .select(split('Input', '\\t').alias('SplitInput')) \\\n",
    "                .select(element_at('SplitInput', 1).alias('Tokens')) \\\n",
    "                .select(explode(split('Tokens', ' ')).alias('Token')) \\\n",
    "                .orderBy('Token') \\\n",
    "                .withColumn('TokenId', monotonically_increasing_id()) \n",
    "        \n",
    "        return token_df\n",
    "\n",
    "    def __create_array_df(self):\n",
    "        n_gram_directories = [os.path.join(self.__root_path, x) for x in os.listdir(self.__root_path) if x.isdigit()]\n",
    "        \n",
    "        input_df = None\n",
    "\n",
    "        for path in n_gram_directories:\n",
    "            new_input_df = spark.read.csv(path, sep='\\n').withColumnRenamed('_c0', 'Input')\n",
    "            \n",
    "            if input_df is None:\n",
    "                input_df = new_input_df\n",
    "            else:\n",
    "                input_df = input_df.union(new_input_df)\n",
    "\n",
    "        split_df = input_df \\\n",
    "                    .select(split('Input', '\\t').alias('SplitInput')) \\\n",
    "                    .select(element_at('SplitInput', 1).alias('Tokens'),\n",
    "                            slice('SplitInput', 2, size('SplitInput')).alias('Data')) \\\n",
    "                    .select(split('Tokens', ' ').alias('Tokens'), 'Data')\n",
    "\n",
    "        array_df = split_df.select('Tokens', transform('Data', lambda d: split(d, ',')).alias('Data')) \\\n",
    "                    .select('Tokens', transform('Data', lambda x: x[0].cast(IntegerType())).alias('Years'),\n",
    "                            transform('Data', lambda x: x[1].cast(LongType())).alias('Frequency'),\n",
    "                            transform('Data', lambda x: x[2].cast(LongType())).alias('BookFrequency')) \\\n",
    "                    .withColumn('NgramId', monotonically_increasing_id())\n",
    "\n",
    "        return array_df\n",
    "\n",
    "    def __create_contains_df(self):\n",
    "        n_gram_df = self.__array_df\n",
    "\n",
    "        n_gram_to_token_id_df = n_gram_df.select('NgramId', 'Tokens') \\\n",
    "                .select(explode('Tokens').alias('Token'), 'NgramId') \\\n",
    "                .join(self.__token_df, on='Token') \\\n",
    "                .groupBy('NgramId').agg(collect_list('TokenId').alias('TokenIds'))\n",
    "\n",
    "        contains_df = n_gram_to_token_id_df.select('NgramId', 'TokenIds') \\\n",
    "            .withColumn('IndexArray', transform('TokenIds', lambda x, i: i)) \\\n",
    "            .select('NgramId', arrays_zip('IndexArray', 'TokenIds').alias('TokenIds')) \\\n",
    "            .select('NgramId', explode('TokenIds').alias('TokenId')) \\\n",
    "            .select('NgramId', 'TokenId.IndexArray', 'TokenId.TokenIds') \\\n",
    "            .withColumnsRenamed({'IndexArray': 'Position', 'TokenIds': 'TokenId'}) \\\n",
    "            .orderBy('NgramId')\n",
    "\n",
    "        return contains_df\n",
    "\n",
    "    ## This horrific arrays to list of structs to map construct is required, because map_from_arrays zeroes everything out.\n",
    "    def __create_data_df(self):\n",
    "        data_df = self.__array_df.select('NgramId', 'Years', 'Frequency', 'BookFrequency')\n",
    "        data_df = data_df.withColumn('FrequencyStructs', arrays_zip('Years', 'Frequency'))\n",
    "        data_df = data_df.withColumn('BookFrequencyStructs', arrays_zip('Years', 'BookFrequency'))\n",
    "        data_df = data_df.withColumn('FrequencyMap', map_from_entries('FrequencyStructs'))\n",
    "        data_df = data_df.withColumn('BookFrequencyMap', map_from_entries('BookFrequencyStructs'))\n",
    "        data_df = data_df.select('NgramId', 'FrequencyMap', 'BookFrequencyMap')\n",
    "\n",
    "        data_df.printSchema()\n",
    "        \n",
    "        return data_df.withColumnsRenamed({'FrequencyMap': 'Frequency', 'BookFrequencyMap': 'BookFrequency'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading \"array.parquet\" ...\n",
      "Loading \"token.parquet\" ...\n",
      "Loading \"contains.parquet\" ...\n",
      "Loading \"data.parquet\" ...\n",
      "root\n",
      " |-- Token: string (nullable = true)\n",
      " |-- TokenId: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- NgramId: long (nullable = true)\n",
      " |-- Position: integer (nullable = true)\n",
      " |-- TokenId: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Years: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- Frequency: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- BookFrequency: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- NgramId: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- NgramId: long (nullable = true)\n",
      " |-- Frequency: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: long (valueContainsNull = true)\n",
      " |-- BookFrequency: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: long (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cl = CorpusLoader('C:/Users/bincl/BA-Thesis/Dataset/parquets_corpus/', spark)\n",
    "\n",
    "cl.load()\n",
    "\n",
    "cl._CorpusLoader__token_df.printSchema()\n",
    "cl._CorpusLoader__contains_df.printSchema()\n",
    "cl._CorpusLoader__array_df.printSchema()\n",
    "cl._CorpusLoader__data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2016: 7, 2017: 3, 2018: 7, 2019: 2, 1990: 1, 1964: 1, 1997: 1, 1999: 1, 2002: 1, 1845: 1, 2005: 2, 2009: 4, 1978: 1, 2010: 1, 2011: 4, 2012: 3, 2013: 7, 2014: 11, 2015: 7}\n",
      "+-------------+--------------------+--------------------+\n",
      "|      NgramId|           Frequency|       BookFrequency|\n",
      "+-------------+--------------------+--------------------+\n",
      "|1391569588591|{1845 -> 1, 1964 ...|{1845 -> 1, 1964 ...|\n",
      "+-------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_last = cl._CorpusLoader__data_df.where('NgramId == \"1391569588591\"')\n",
    "print(tokens_last.first()[\"Frequency\"])\n",
    "tokens_last.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+\n",
      "|    NgramId|Position|TokenId|\n",
      "+-----------+--------+-------+\n",
      "| 8589935889|       0|  11890|\n",
      "|34359740934|       1|  11890|\n",
      "|34359743488|       1|  11890|\n",
      "|34359743876|       1|  11890|\n",
      "|34359745277|       1|  11890|\n",
      "|34359749969|       1|  11890|\n",
      "|34359750873|       1|  11890|\n",
      "|34359760437|       1|  11890|\n",
      "|34359776562|       1|  11890|\n",
      "|34359779021|       1|  11890|\n",
      "|34359785558|       1|  11890|\n",
      "|34359785747|       1|  11890|\n",
      "|34359790758|       1|  11890|\n",
      "|34359791432|       1|  11890|\n",
      "|34359793582|       1|  11890|\n",
      "|34359797169|       1|  11890|\n",
      "|34359797826|       1|  11890|\n",
      "|34359801879|       1|  11890|\n",
      "|34359806613|       1|  11890|\n",
      "|34359812129|       1|  11890|\n",
      "+-----------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_last = cl._CorpusLoader__contains_df.where('TokenId == \"11890\"')\n",
    "tokens_last.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
