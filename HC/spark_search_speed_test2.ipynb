{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col, element_at, slice, size, regexp_extract, transform, when, explode, \\\n",
    "monotonically_increasing_id, map_from_arrays, lit, udf,collect_list, row_number, ceil, map_keys, expr, from_json\n",
    "from pyspark.sql.types import ShortType, ArrayType, LongType, StringType\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import zscore\n",
    "import gzip\n",
    "import os\n",
    "import csv\n",
    "from alive_progress import alive_bar\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "directory = 'C:/Users/bincl/BA-Thesis/Dataset/2gram/'\n",
    "\n",
    "def find_n_gram(direc, find):\n",
    "    for filename in os.listdir(direc):\n",
    "        f = os.path.join(direc, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "            print(f)\n",
    "            with gzip.open(f,'rt', encoding='utf-8') as input:\n",
    "                str_arr_csv = input.readlines()\n",
    "                for line in str_arr_csv:\n",
    "                    token = line.split(\"\\t\")\n",
    "                    if find == token[0]:\n",
    "                        return line\n",
    "                        \n",
    "    return \"line not found\" \n",
    "\n",
    "def start_the_search(directory,search):\n",
    "    result = find_n_gram(directory,search)\n",
    "    \n",
    "    if result == \"line not found\":\n",
    "        print(result)\n",
    "        exit()\n",
    "    result = result.split(\"\\t\")\n",
    "    lower_ngram = pd.Series(result[1:],dtype= \"string\", name = result[0])\n",
    "    return lower_ngram\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_aa.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_ab.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_ac.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_ad.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_ae.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_af.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_ag.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_ah.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_ai.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_aj.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_ak.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_al.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_am.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_an.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_ao.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_ap.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_aq.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_ar.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_as.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_at.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_au.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_av.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_aw.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_ax.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_ay.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_az.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_ba.gz\n",
      "C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_bb.gz\n",
      "line not found\n",
      "Series([], Name: line not found, dtype: string)\n"
     ]
    }
   ],
   "source": [
    "found = start_the_search(directory,\"Geschlecht '\")\n",
    "print(found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col, element_at, slice, size, regexp_extract, transform, when, explode, \\\n",
    "monotonically_increasing_id, map_from_arrays, lit, udf,collect_list, row_number, ceil, map_keys, expr, from_json\n",
    "from pyspark.sql.types import ShortType, ArrayType, LongType, StringType\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "years_Columns = list(range(1800,2000)) \n",
    "directory = 'C:/Users/bincl/BA-Thesis/Dataset/1gram/1_20000_nopos.gz'\n",
    "\n",
    "#pathlist = [directory + f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "\n",
    "spark2 = SparkSession.builder.appName('3gramSQL').getOrCreate()\n",
    "\n",
    "raw_input_df2 = spark2 \\\n",
    "                .read.csv(directory, sep='\\n',quote=\"\").withColumnRenamed('_c0', 'Input')\n",
    "\n",
    "split_df2 = raw_input_df2 \\\n",
    "                .select(split('Input', '\\t').alias('SplitInput')) \\\n",
    "                .select(element_at('SplitInput', 1).alias('Tokens'),\n",
    "                        slice('SplitInput', 2,\n",
    "size('SplitInput')).alias('Data')) \\\n",
    "                .select('Tokens', 'Data') \\\n",
    "\n",
    "df_2gram = split_df2.select('Tokens', transform('Data', lambda d:\n",
    "split(d, ',')).alias('Data')) \\\n",
    "                .select('Tokens', transform('Data', lambda x:\n",
    "x[0]).alias('Year'),\n",
    "                        transform('Data', lambda x:\n",
    "x[1]).cast(ArrayType(LongType())).alias('Occurrences')) \\\n",
    "                .select('Tokens', map_from_arrays('Year',\n",
    "'Occurrences').alias('Data')) \\\n",
    "                .withColumn('Id', monotonically_increasing_id()) \\\n",
    "                .select(['Id', 'Tokens', 'Data'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def get_pd_df_from_string(suchstring, df):\n",
    "    x = df_2gram\n",
    "    x = x.where(x.Tokens == suchstring)\n",
    "    id = x.first()['Id']\n",
    "    print(id)\n",
    "    return id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    }
   ],
   "source": [
    "matched_pandas_df = get_pd_df_from_string('Bis', df_2gram)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
