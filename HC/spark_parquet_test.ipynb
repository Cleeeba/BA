{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col, element_at, slice, size, regexp_extract, transform, when, explode, \\\n",
    "monotonically_increasing_id, map_from_arrays, lit, udf,collect_list, row_number, ceil, map_keys, expr, from_json\n",
    "from pyspark.sql.types import ShortType, ArrayType, LongType, StringType\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import zscore\n",
    "import gzip\n",
    "import os\n",
    "import csv\n",
    "from alive_progress import alive_bar\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col, element_at, slice, size, regexp_extract, transform, when, explode, \\\n",
    "monotonically_increasing_id, map_from_arrays, lit, udf,collect_list, row_number, ceil, map_keys, expr, from_json\n",
    "from pyspark.sql.types import ShortType, ArrayType, LongType, StringType\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import zscore\n",
    "import gzip\n",
    "import os\n",
    "import csv\n",
    "from alive_progress import alive_bar\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "\n",
    "from spark_sql_to_sqlite import spark_sql_to_sqlite\n",
    "\n",
    "directory2 ='C:/Users/bincl/BA-Thesis/Dataset/2gram/parquet/aa.gz.parquet'\n",
    "\n",
    "directory = 'C:/Users/bincl/BA-Thesis/Dataset/2gram/default/2_20000_nopos_aa.gz'\n",
    "\n",
    "\n",
    "        # checking if it is a file\n",
    "spark = SparkSession.builder.appName('3gramSQL').getOrCreate()\n",
    "\n",
    "raw_input_df = spark \\\n",
    "        .read.csv(directory, sep='\\n',quote=\"\").withColumnRenamed('_c0', 'Input')\n",
    "\n",
    "split_df = raw_input_df \\\n",
    "                        .select(split('Input', '\\t').alias('SplitInput')) \\\n",
    "                        .select(element_at('SplitInput', 1).alias('Tokens'),\n",
    "                                slice('SplitInput', 2,\n",
    "        size('SplitInput')).alias('Data')) \\\n",
    "                        .select('Tokens', 'Data') \\\n",
    "\n",
    "df_2gram = split_df.select('Tokens', transform('Data', lambda d:\n",
    "        split(d, ',')).alias('Data')) \\\n",
    "                        .select('Tokens', transform('Data', lambda x:\n",
    "        x[0]).alias('Year'),\n",
    "                                transform('Data', lambda x:\n",
    "        x[1]).cast(ArrayType(LongType())).alias('Occurrences')) \\\n",
    "                        .select('Tokens', map_from_arrays('Year',\n",
    "        'Occurrences').alias('Data')) \\\n",
    "                        .select('Tokens', 'Data')  \n",
    "df_2gram.write.parquet   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = list(range(0,2000)) \n",
    "years_Columns = list(range(1800,2000)) \n",
    "names = ['token']\n",
    "names = names + numbers\n",
    "\n",
    "df = pd.read_csv(directory, compression= 'gzip', header=None, dtype= str, index_col= [0],names=names,sep='\\t',engine='python', quoting=csv.QUOTE_NONE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd \n",
    "\n",
    "dd.read_parquet(r\"C:/Users/bincl/BA-Thesis/Dataset/2gram/parquet/aa.gz.parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
