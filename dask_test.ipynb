{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"An error occurred while calling the read_parquet method registered to the pandas backend.\\nOriginal Message: 'None of [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None] are in the columns'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\bincl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\backends.py:136\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\bincl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py:538\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, columns, filters, categories, index, storage_options, engine, use_nullable_dtypes, dtype_backend, calculate_divisions, ignore_metadata_file, metadata_task_size, split_row_groups, blocksize, aggregate_files, parquet_file_extension, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m     blocksize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m read_metadata_result \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgather_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalculate_divisions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_row_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_row_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregate_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregate_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_metadata_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_metadata_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_task_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_task_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparquet_file_extension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_file_extension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;66;03m# In the future, we may want to give the engine the\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# option to return a dedicated element for `common_kwargs`.\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;66;03m# However, to avoid breaking the API, we just embed this\u001b[39;00m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;66;03m# data in the first element of `parts` for now.\u001b[39;00m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# The logic below is inteded to handle backward and forward\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# compatibility with a user-defined engine.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bincl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py:553\u001b[0m, in \u001b[0;36mArrowDatasetEngine.read_metadata\u001b[1;34m(cls, fs, paths, categories, index, use_nullable_dtypes, dtype_backend, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;66;03m# Stage 2: Generate output `meta`\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dd_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# Stage 3: Generate parts and stats\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bincl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py:1234\u001b[0m, in \u001b[0;36mArrowDatasetEngine._create_dd_meta\u001b[1;34m(cls, dataset_info)\u001b[0m\n\u001b[0;32m   1233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_cols \u001b[38;5;129;01mand\u001b[39;00m index_cols \u001b[38;5;241m!=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[1;32m-> 1234\u001b[0m     \u001b[43mmeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Ensure that there is no overlap between partition columns\u001b[39;00m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;66;03m# and explicit column storage\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bincl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bincl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:6001\u001b[0m, in \u001b[0;36mDataFrame.set_index\u001b[1;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[0;32m   6000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[1;32m-> 6001\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are in the columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'None of [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None] are in the columns'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#DaskDf = pd.read_parquet('C:/Users/bincl/BA-Thesis/BA/HC/bd.gz.parquet')\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m DaskDf \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/bincl/BA-Thesis/BA/HC/bd.gz.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#DaskDf = dd.read_parquet('C:/Users/bincl/BA-Thesis/BA/HC/bd.gz.parquet', header=0, sep='\\t', quotechar='\"', on_bad_lines='skip', quoting=csv.QUOTE_NONE)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(DaskDf)    \n",
      "File \u001b[1;32mc:\\Users\\bincl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\backends.py:138\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname(func)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod registered to the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m backend.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"An error occurred while calling the read_parquet method registered to the pandas backend.\\nOriginal Message: 'None of [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None] are in the columns'\""
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import zscore\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "#DaskDf = pd.read_parquet('C:/Users/bincl/BA-Thesis/BA/HC/bd.gz.parquet')\n",
    "DaskDf = dd.read_parquet('C:/Users/bincl/BA-Thesis/BA/HC/bd.gz.parquet')\n",
    "#DaskDf = dd.read_parquet('C:/Users/bincl/BA-Thesis/BA/HC/bd.gz.parquet', header=0, sep='\\t', quotechar='\"', on_bad_lines='skip', quoting=csv.QUOTE_NONE)\n",
    "    \n",
    "print(DaskDf)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mDaskDf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnpartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "DaskDf.npartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandaDf = pd.read_csv('C:/Users/bincl/BA-Thesis/Dataset/2gram/2_20000_nopos_aa.gz', compression='gzip', header=0, sep='\\t', quotechar='\"', on_bad_lines='skip', quoting=csv.QUOTE_NONE)\n",
    "#print(pandaDf)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bincl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:546: UserWarning: Warning gzip compression does not support breaking apart files\n",
      "Please ensure that each individual file can fit in memory and\n",
      "use the keyword ``blocksize=None to remove this message``\n",
      "Setting ``blocksize=None``\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "df1gram = dd.read_csv('C:/Users/bincl/BA-Thesis/Dataset/1gram/1_20000_nopos.gz', compression='gzip', header=0, sep='\\t', quotechar='\"', on_bad_lines='skip', quoting=csv.QUOTE_NONE)\n",
    "df2gram = DaskDf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLowerLevelString(string):\n",
    "   first, *middle, last = string.split()\n",
    "   first = first + ''.join(middle)\n",
    "   last = ''.join(middle) + last\n",
    "   return first, last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLR(token):\n",
    "    first,last = getLowerLevelString(token)\n",
    "    X = dd.concat([df1gram.loc[first],df1gram.loc[last]],axis=1)\n",
    "    y = df2gram.loc[token]\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X, y) \n",
    "    \n",
    "    return reg.coef_,reg.intercept_,X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildApproximation(c1,c2,basevalue,df):\n",
    "    df = df.astype('float')\n",
    "    df['scaledFirst'] = df.iloc[:,0].apply(lambda x: x * c1) \n",
    "    df['scaledLast'] = df.iloc[:,1].apply(lambda x: x * c2) \n",
    "    df['approximation'] = df['scaledFirst'] + df['scaledLast'] + basevalue\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compressWithError2gram(firstN,error):\n",
    "    df_N_2gram = df2gram.head(firstN)\n",
    "    \n",
    "    result = []\n",
    "    all = []\n",
    "    sum = []\n",
    "    for ngram in df_N_2gram.index.values:\n",
    "        coef,intercept,dfAprox = MLR(ngram)\n",
    "        c1,c2 = coef\n",
    "        df = buildApproximation(c1,c2,intercept,dfAprox)\n",
    "        dfOriginal = dd.DataFrame()\n",
    "        dfOriginal['values'] = df2gram.loc[ngram]\n",
    "        dfOriginal['values'] = dfOriginal['values'].apply(pd.to_numeric, errors='coerce')\n",
    "        #dfOriginal= pd.to_numeric(dfOriginal)\n",
    "        dfOriginal['zscore'] = zscore(dfOriginal)\n",
    "        df['zscore'] = zscore(df['approximation'])\n",
    "        \n",
    "        sum.append(dd.to_numeric(dfOriginal['values']).sum()) \n",
    "        \n",
    "        rmse = mean_squared_error(dfOriginal['zscore'], df['zscore'], squared = False)\n",
    "        #rmse = mean_squared_error(dfOriginal, df['approximation'])\n",
    "        if rmse <= error:\n",
    "            result.append([ngram,rmse,dfOriginal['values'],dfOriginal['zscore'],df['approximation'],df['zscore']]) \n",
    "        all.append(rmse)    \n",
    "    return result, all, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result, \u001b[38;5;28mall\u001b[39m, \u001b[38;5;28msum\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcompressWithError2gram\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(result))\n",
      "Cell \u001b[1;32mIn[35], line 8\u001b[0m, in \u001b[0;36mcompressWithError2gram\u001b[1;34m(firstN, error)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28msum\u001b[39m \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ngram \u001b[38;5;129;01min\u001b[39;00m df_N_2gram\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalues:\n\u001b[1;32m----> 8\u001b[0m     coef,intercept,dfAprox \u001b[38;5;241m=\u001b[39m \u001b[43mMLR\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     c1,c2 \u001b[38;5;241m=\u001b[39m coef\n\u001b[0;32m     10\u001b[0m     df \u001b[38;5;241m=\u001b[39m buildApproximation(c1,c2,intercept,dfAprox)\n",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m, in \u001b[0;36mMLR\u001b[1;34m(token)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mMLR\u001b[39m(token):\n\u001b[1;32m----> 2\u001b[0m     first,last \u001b[38;5;241m=\u001b[39m \u001b[43mgetLowerLevelString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     X \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mconcat([df1gram\u001b[38;5;241m.\u001b[39mloc[first],df1gram\u001b[38;5;241m.\u001b[39mloc[last]],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m     y \u001b[38;5;241m=\u001b[39m df2gram\u001b[38;5;241m.\u001b[39mloc[token]\n",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m, in \u001b[0;36mgetLowerLevelString\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetLowerLevelString\u001b[39m(string):\n\u001b[1;32m----> 2\u001b[0m    first, \u001b[38;5;241m*\u001b[39mmiddle, last \u001b[38;5;241m=\u001b[39m \u001b[43mstring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m()\n\u001b[0;32m      3\u001b[0m    first \u001b[38;5;241m=\u001b[39m first \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(middle)\n\u001b[0;32m      4\u001b[0m    last \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(middle) \u001b[38;5;241m+\u001b[39m last\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "result, all, sum = compressWithError2gram(10000,0.05)\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = 1\n",
    "\n",
    "plt.ticklabel_format(style = 'plain')\n",
    "result[graph][2].plot()\n",
    "plt.show()\n",
    "plt.ticklabel_format(style = 'plain')\n",
    "result[graph][4].plot()\n",
    "plt.show()\n",
    "result[graph][3].plot()\n",
    "plt.show()\n",
    "result[graph][5].plot()\n",
    "plt.show()\n",
    "\n",
    "print(result[graph][1])\n",
    "print(result[graph][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
